{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Dataset class for images\n",
    "class ActionImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        for label, action in enumerate(self.classes):\n",
    "            action_dir = os.path.join(root_dir, action)\n",
    "            for img_file in os.listdir(action_dir):\n",
    "                img_path = os.path.join(action_dir, img_file)\n",
    "                self.data.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = Image.fromarray(img)  # Convert NumPy array to PIL image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Dataset class for videos\n",
    "# class ActionVideoDataset(Dataset):\n",
    "#     def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "#         self.video_dir = video_dir\n",
    "#         self.num_frames = num_frames\n",
    "#         self.transform = transform\n",
    "#         self.video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.video_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         video_path = self.video_files[idx]\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         frames = []\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             frame = Image.fromarray(frame)  # Convert NumPy array to PIL image\n",
    "#             if self.transform:\n",
    "#                 frame = self.transform(frame)\n",
    "#             frames.append(frame)\n",
    "#         cap.release()\n",
    "#         # Pad missing frames with zeros if video is too short\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             frames.append(torch.zeros_like(frames[0]))\n",
    "#         frames = torch.stack(frames)\n",
    "#         return frames, video_path\n",
    "# class ActionVideoDataset(Dataset):\n",
    "#     def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "#         self.video_dir = video_dir\n",
    "#         self.num_frames = num_frames\n",
    "#         self.transform = transform\n",
    "#         self.classes = os.listdir(video_dir)  # List of action folder names\n",
    "#         self.label_to_class = {i: cls for i, cls in enumerate(self.classes)}  # Map index to folder name\n",
    "#         self.video_files = [(os.path.join(video_dir, cls, f), i) \n",
    "#                             for i, cls in enumerate(self.classes) \n",
    "#                             for f in os.listdir(os.path.join(video_dir, cls)) if f.endswith('.mp4')]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.video_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         video_path, label = self.video_files[idx]\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         frames = []\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             frame = Image.fromarray(frame)\n",
    "#             if self.transform:\n",
    "#                 frame = self.transform(frame)\n",
    "#             frames.append(frame)\n",
    "#         cap.release()\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             frames.append(torch.zeros_like(frames[0]))\n",
    "#         frames = torch.stack(frames)\n",
    "#         return frames, label, video_path\n",
    "\n",
    "class ActionVideoDatasetSingle(Dataset):\n",
    "    def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(video_dir)  # List of action folder names\n",
    "        self.label_to_class = {i: cls for i, cls in enumerate(self.classes)}  # Map index to folder name\n",
    "\n",
    "        # Select one video per action label folder\n",
    "        self.video_files = []\n",
    "        for label, cls in enumerate(self.classes):\n",
    "            action_folder = os.path.join(video_dir, cls)\n",
    "            videos = [f for f in os.listdir(action_folder) if f.endswith('.mp4')]\n",
    "            if videos:\n",
    "                # Pick the first video from the folder (or a random video)\n",
    "                self.video_files.append((os.path.join(action_folder, videos[0]), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_files[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while len(frames) < self.num_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)  # Convert NumPy array to PIL image\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # Pad missing frames with zeros if the video is too short\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label, video_path\n",
    "\n",
    "\n",
    "# Transforms\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "video_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ActionImageDataset(root_dir='action_sp', transform=image_transform)\n",
    "test_dataset = ActionVideoDatasetSingle(video_dir='resized_videos', transform=video_transform)\n",
    "\n",
    "# Subset to train on 50 images\n",
    "# train_subset = Subset(train_dataset, list(range(200)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Test loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "# from torchvision.transforms import Compose, ToTensor, Resize, Normalize, RandomHorizontalFlip, RandomRotation, ColorJitter\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "# # Dataset class for images\n",
    "# class ActionImageDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.data = []\n",
    "#         self.labels = []\n",
    "#         self.classes = os.listdir(root_dir)\n",
    "#         for label, action in enumerate(self.classes):\n",
    "#             action_dir = os.path.join(root_dir, action)\n",
    "#             for img_file in os.listdir(action_dir):\n",
    "#                 img_path = os.path.join(action_dir, img_file)\n",
    "#                 self.data.append(img_path)\n",
    "#                 self.labels.append(label)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.data[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         img = cv2.imread(img_path)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "#         img = Image.fromarray(img)  # Convert NumPy array to PIL image\n",
    "#         if self.transform:\n",
    "#             img = self.transform(img)\n",
    "#         return img, label\n",
    "\n",
    "\n",
    "# # Dataset class for videos\n",
    "# class ActionVideoDatasetSingle(Dataset):\n",
    "#     def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "#         self.video_dir = video_dir\n",
    "#         self.num_frames = num_frames\n",
    "#         self.transform = transform\n",
    "#         self.classes = os.listdir(video_dir)  # List of action folder names\n",
    "#         self.label_to_class = {i: cls for i, cls in enumerate(self.classes)}  # Map index to folder name\n",
    "\n",
    "#         # Select one video per action label folder\n",
    "#         self.video_files = []\n",
    "#         for label, cls in enumerate(self.classes):\n",
    "#             action_folder = os.path.join(video_dir, cls)\n",
    "#             videos = [f for f in os.listdir(action_folder) if f.endswith('.mp4')]\n",
    "#             if videos:\n",
    "#                 # Pick the first video from the folder (or a random video)\n",
    "#                 self.video_files.append((os.path.join(action_folder, videos[0]), label))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.video_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         video_path, label = self.video_files[idx]\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         frames = []\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             frame = Image.fromarray(frame)  # Convert NumPy array to PIL image\n",
    "#             if self.transform:\n",
    "#                 frame = self.transform(frame)\n",
    "#             frames.append(frame)\n",
    "#         cap.release()\n",
    "\n",
    "#         # Pad missing frames with zeros if the video is too short\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             frames.append(torch.zeros_like(frames[0]))\n",
    "#         frames = torch.stack(frames)\n",
    "#         return frames, label, video_path\n",
    "\n",
    "\n",
    "# # Augmented transforms\n",
    "# augmented_transform = Compose([\n",
    "#     Resize((224, 224)),\n",
    "#     RandomHorizontalFlip(),\n",
    "#     RandomRotation(15),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     ToTensor(),\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# # Regular transforms for test dataset\n",
    "# video_transform = Compose([\n",
    "#     Resize((224, 224)),\n",
    "#     ToTensor(),\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# # Load datasets\n",
    "# train_dataset = ActionImageDataset(root_dir='action_sp', transform=augmented_transform)\n",
    "# test_dataset = ActionVideoDatasetSingle(video_dir='resized_videos', transform=video_transform)\n",
    "\n",
    "# # Class balancing with WeightedRandomSampler\n",
    "# class_counts = Counter(train_dataset.labels)\n",
    "# total_samples = sum(class_counts.values())\n",
    "# class_weights = {label: total_samples / count for label, count in class_counts.items()}\n",
    "\n",
    "# # Create a subset of the dataset\n",
    "# train_subset = Subset(train_dataset, list(range(200)))\n",
    "\n",
    "# # Adjust weights for the subset\n",
    "# sample_weights = [class_weights[train_dataset.labels[idx]] for idx in train_subset.indices]\n",
    "# sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_subset), replacement=True)\n",
    "\n",
    "# # DataLoader with sampler for training\n",
    "# train_loader = DataLoader(train_subset, batch_size=14, sampler=sampler)\n",
    "\n",
    "\n",
    "# # Test loader\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n",
    "# print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class RViT(nn.Module):\n",
    "#     def __init__(self, num_classes, hidden_dim, num_layers, frame_dim):\n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "    #     # Patch embedding with reduced stride and kernel size\n",
    "    #     # self.patch_embedding = nn.Conv3d(16, hidden_dim, kernel_size=(3, 8, 8), stride=(3, 4, 4), padding=(1, 2, 2))\n",
    "    #     self.patch_embedding = nn.Conv3d(3, hidden_dim, kernel_size=(3, 8, 8), stride=(3, 4, 4), padding=(1, 2, 2))\n",
    "\n",
    "    #     # Simplified learnable position encoding\n",
    "    #     # Adjust the temporal dimension to match the expected input\n",
    "    #     self.position_encoding = nn.Parameter(torch.randn(1, hidden_dim, 1, 56, 56), requires_grad=True)\n",
    "        \n",
    "    #     self.rvit_units = nn.ModuleList([RViTUnit(hidden_dim) for _ in range(num_layers)])\n",
    "    #     self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    #     self.frame_reconstruction = nn.Sequential(\n",
    "    #         nn.Conv3d(hidden_dim, 64, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv3d(64, frame_dim[0], kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "    #     )\n",
    "    #     self.temporal_upsample = nn.Upsample(size=(15, 224, 224), mode='trilinear', align_corners=False)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Patch embedding\n",
    "    #     patches = self.patch_embedding(x)  # Shape: [batch_size, hidden_dim, depth, height, width]\n",
    "    #     # print(\"After patch embedding:\", patches.shape)\n",
    "        \n",
    "    #     # Dynamically resize position encoding to match patches' spatial dimensions\n",
    "    #     _, _, depth, height, width = patches.shape\n",
    "    #     pos_encoding = F.interpolate(\n",
    "    #         self.position_encoding, size=(depth, height, width), mode='trilinear', align_corners=False\n",
    "    #     )  # Adjusted shape: [1, hidden_dim, depth, height, width]\n",
    "    #     # print(\"Position encoding shape after interpolation:\", pos_encoding.shape)\n",
    "        \n",
    "    #     # Add position encoding\n",
    "        # patches += pos_encoding\n",
    "        # # print(\"After adding position encoding:\", patches.shape)\n",
    "\n",
    "        # # Initialize recurrent state\n",
    "        # h = torch.zeros_like(patches)\n",
    "        # # print(\"Initialized recurrent state:\", h.shape)\n",
    "\n",
    "        # # Pass through RViT units\n",
    "        # for i, unit in enumerate(self.rvit_units):\n",
    "        #     h = unit(patches, h)\n",
    "        #     # print(f\"After RViT unit {i+1}:\", h.shape)\n",
    "\n",
    "        # # Classification\n",
    "        # h_last = h.mean(dim=(2, 3, 4))  # Global average pooling over spatial dimensions\n",
    "        # action_logits = self.classifier(h_last)\n",
    "        # print(\"Action logits shape:\", action_logits.shape)\n",
    "\n",
    "        # # Frame reconstruction\n",
    "        # reconstructed_frame = self.frame_reconstruction(h)\n",
    "        # # print(\"Reconstructed frame shape:\", reconstructed_frame.shape)\n",
    "\n",
    "        # return action_logits, reconstructed_frame\n",
    "\n",
    "class RViT(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim, num_layers, frame_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embedding = nn.Conv3d(3, hidden_dim, kernel_size=(3, 8, 8), stride=(3, 4, 4), padding=(1, 2, 2))\n",
    "        self.position_encoding = nn.Parameter(torch.randn(1, hidden_dim, 1, 56, 56), requires_grad=True)\n",
    "\n",
    "        # Attention layers\n",
    "        self.scaled_attention = ScaledDotProductAttention(hidden_dim)\n",
    "        self.linear_attention = LinearAttention(hidden_dim)\n",
    "\n",
    "        # Recurrent Vision Transformer Units\n",
    "        self.rvit_units = nn.ModuleList([RViTUnit(hidden_dim) for _ in range(num_layers)])\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch embedding\n",
    "        patches = self.patch_embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        _, _, depth, height, width = patches.shape\n",
    "        pos_encoding = F.interpolate(self.position_encoding, size=(depth, height, width), mode='trilinear', align_corners=False)\n",
    "        patches += pos_encoding\n",
    "\n",
    "        # Attention mechanisms\n",
    "        scaled_attn_output = self.scaled_attention(patches, patches)\n",
    "        lin_attn_output = self.linear_attention(scaled_attn_output, scaled_attn_output)\n",
    "\n",
    "        # Recurrent processing\n",
    "        h = torch.zeros_like(lin_attn_output).to(lin_attn_output.device)\n",
    "        for unit in self.rvit_units:\n",
    "            h = unit(lin_attn_output, h)\n",
    "\n",
    "        # Classification\n",
    "        h_last = h.mean(dim=(2, 3, 4))  # Global average pooling\n",
    "        logits = self.classifier(h_last)\n",
    "        return logits, h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wk = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wv = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Compute queries, keys, and values\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(h)\n",
    "        v = self.Wv(h)\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn = torch.softmax((q * k).sum(dim=1, keepdim=True) * self.scale, dim=-1)\n",
    "        output = attn * v  # Apply attention weights to values\n",
    "        \n",
    "        return output\n",
    "        \n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wk = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wv = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wo = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Compute queries, keys, and values\n",
    "        q = self.Wq(x)  # Shape: [batch_size, hidden_dim, depth, height, width]\n",
    "        k = self.Wk(h)  # Shape: [batch_size, hidden_dim, depth, height, width]\n",
    "        v = self.Wv(h)  # Shape: [batch_size, hidden_dim, depth, height, width]\n",
    "        \n",
    "        # Reshape q and k for attention computation\n",
    "        q = q.flatten(start_dim=2)  # Shape: [batch_size, hidden_dim, depth*height*width]\n",
    "        k = k.flatten(start_dim=2)  # Shape: [batch_size, hidden_dim, depth*height*width]\n",
    "        v = v.flatten(start_dim=2)  # Shape: [batch_size, hidden_dim, depth*height*width]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.bmm(q.transpose(1, 2), k)  # Shape: [batch_size, depth*height*width, depth*height*width]\n",
    "        attn_weights = attn_weights / (k.size(1) ** 0.5)  # Scale by sqrt of hidden_dim\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)  # Apply softmax over last dimension\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attn_output = torch.bmm(v, attn_weights.transpose(1, 2))  # Shape: [batch_size, hidden_dim, depth*height*width]\n",
    "        \n",
    "        # Reshape back to 3D\n",
    "        attn_output = attn_output.view_as(h)  # Shape: [batch_size, hidden_dim, depth, height, width]\n",
    "\n",
    "        # Final projection to match the input shape\n",
    "        output = self.Wo(attn_output)  # Shape: [batch_size, hidden_dim, depth, height, width]\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class RViTUnit(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_rate=0.5\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.attention_gate = LinearAttention(hidden_dim)  # Use LinearAttention here\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "        self.recurrent_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Attention mechanism with linear attention\n",
    "        attn_output = self.attention_gate(x, h)\n",
    "        attn_output = self.recurrent_dropout(attn_output)\n",
    "    \n",
    "        # Reshape for LayerNorm\n",
    "        batch_size, hidden_dim, depth, height, width = attn_output.shape\n",
    "        h_flat = h.permute(0, 2, 3, 4, 1).reshape(-1, hidden_dim)\n",
    "        attn_output_flat = attn_output.permute(0, 2, 3, 4, 1).reshape(-1, hidden_dim)\n",
    "    \n",
    "        # Apply LayerNorm\n",
    "        h_new_flat = self.layer_norm1(h_flat + attn_output_flat)\n",
    "    \n",
    "        # Reshape back to original shape\n",
    "        h_new = h_new_flat.reshape(batch_size, depth, height, width, hidden_dim).permute(0, 4, 1, 2, 3)\n",
    "    \n",
    "        # Apply FFN with LayerNorm\n",
    "        h_new_flat = h_new.permute(0, 2, 3, 4, 1).reshape(-1, hidden_dim)\n",
    "        h_new_flat = self.layer_norm2(h_new_flat + self.ffn(h_new_flat))\n",
    "    \n",
    "        h_new = h_new_flat.reshape(batch_size, depth, height, width, hidden_dim).permute(0, 4, 1, 2, 3)\n",
    "        return h_new\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#new trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, ColorJitter, Resize, ToTensor, Normalize\n",
    "\n",
    "# Dataset class for images\n",
    "class ActionImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_count=None, augment=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.augment = augment\n",
    "\n",
    "        # Count images per class and balance classes with augmentation\n",
    "        if target_count and augment:\n",
    "            self._balance_classes(target_count)\n",
    "\n",
    "        # Load dataset\n",
    "        for label, action in enumerate(self.classes):\n",
    "            action_dir = os.path.join(root_dir, action)\n",
    "            for img_file in os.listdir(action_dir):\n",
    "                img_path = os.path.join(action_dir, img_file)\n",
    "                self.data.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def _balance_classes(self, target_count):\n",
    "        # Data augmentation pipeline\n",
    "        augmentation_transform = Compose([\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            RandomRotation(degrees=15),\n",
    "            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        ])\n",
    "\n",
    "        for label, action in enumerate(self.classes):\n",
    "            action_dir = os.path.join(self.root_dir, action)\n",
    "            image_files = os.listdir(action_dir)\n",
    "            count = len(image_files)\n",
    "            if count < target_count:\n",
    "                print(f\"Augmenting {action} with {target_count - count} new images.\")\n",
    "                for i in range(target_count - count):\n",
    "                    img_path = os.path.join(action_dir, image_files[i % count])\n",
    "                    img = Image.open(img_path)\n",
    "                    \n",
    "                    # Convert image to RGB mode if not already\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    \n",
    "                    # Apply augmentation\n",
    "                    augmented_img = augmentation_transform(img)\n",
    "                    \n",
    "                    # Save augmented image\n",
    "                    augmented_img_path = os.path.join(action_dir, f\"augmented_{i}.jpg\")\n",
    "                    augmented_img.save(augmented_img_path)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Ensure it's in RGB format\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class ActionVideoDatasetSingle(Dataset):\n",
    "    def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(video_dir)  # List of action folder names\n",
    "        self.label_to_class = {i: cls for i, cls in enumerate(self.classes)}  # Map index to folder name\n",
    "\n",
    "        # Select one video per action label folder\n",
    "        self.video_files = []\n",
    "        for label, cls in enumerate(self.classes):\n",
    "            action_folder = os.path.join(video_dir, cls)\n",
    "            videos = [f for f in os.listdir(action_folder) if f.endswith('.mp4')]\n",
    "            if videos:\n",
    "                # Pick the first video from the folder (or a random video)\n",
    "                self.video_files.append((os.path.join(action_folder, videos[0]), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_files[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while len(frames) < self.num_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)  # Convert NumPy array to PIL image\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # Pad missing frames with zeros if the video is too short\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label, video_path\n",
    "\n",
    "\n",
    "# Transforms\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "video_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Parameters for class balancing\n",
    "target_count = 15  # Define the target number of images per class\n",
    "\n",
    "# Load datasets with augmentation for imbalance correction\n",
    "train_dataset = ActionImageDataset(root_dir='action_sp', transform=image_transform, target_count=target_count, augment=True)\n",
    "test_dataset = ActionVideoDatasetSingle(video_dir='resized_videos', transform=video_transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Confirm updated dataset statistics\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'questioning and answering' with 84 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:   5%|▍         | 1/21 [00:00<00:11,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'giving or receiving award' with 81 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  10%|▉         | 2/21 [00:00<00:08,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'wrapping present' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  14%|█▍        | 3/21 [00:01<00:07,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'wrestling' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  19%|█▉        | 4/21 [00:01<00:06,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'checking tires' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  24%|██▍       | 5/21 [00:02<00:06,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'air drumming' with 82 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  29%|██▊       | 6/21 [00:02<00:05,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'playing trombone' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  33%|███▎      | 7/21 [00:02<00:05,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'milking cow' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  38%|███▊      | 8/21 [00:03<00:05,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'applauding' with 82 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  43%|████▎     | 9/21 [00:03<00:04,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'moving furniture' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  48%|████▊     | 10/21 [00:04<00:04,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'abseiling' with 65 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  52%|█████▏    | 11/21 [00:04<00:03,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'pushing wheelchair' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  57%|█████▋    | 12/21 [00:04<00:03,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'riding elephant' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  62%|██████▏   | 13/21 [00:05<00:03,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'opening bottle' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  67%|██████▋   | 14/21 [00:05<00:02,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'playing harp' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  71%|███████▏  | 15/21 [00:06<00:02,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'applying cream' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  76%|███████▌  | 16/21 [00:06<00:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'throwing axe' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  81%|████████  | 17/21 [00:06<00:01,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'archery' with 81 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  86%|████████▌ | 18/21 [00:07<00:01,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'doing aerobics' with 80 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  90%|█████████ | 19/21 [00:07<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'juggling soccer ball' with 81 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes:  95%|█████████▌| 20/21 [00:08<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class 'waxing chest' with 85 new images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing Classes: 100%|██████████| 21/21 [00:08<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final image counts per class: {'questioning and answering': 100, 'giving or receiving award': 100, 'wrapping present': 100, 'wrestling': 100, 'checking tires': 100, 'air drumming': 100, 'playing trombone': 100, 'milking cow': 100, 'applauding': 100, 'moving furniture': 100, 'abseiling': 100, 'pushing wheelchair': 100, 'riding elephant': 100, 'opening bottle': 100, 'playing harp': 100, 'applying cream': 100, 'throwing axe': 100, 'archery': 100, 'doing aerobics': 100, 'juggling soccer ball': 100, 'waxing chest': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    Compose, RandomHorizontalFlip, RandomRotation, ColorJitter,\n",
    "    RandomResizedCrop, GaussianBlur, RandomPerspective\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BalancedAugmentor:\n",
    "    def __init__(self, root_dir, target_count=100, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the original dataset root directory.\n",
    "            target_count (int): Desired number of images per action label after augmentation.\n",
    "            transform (callable, optional): Transform to apply for augmentation.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_count = target_count\n",
    "        self.transform = transform if transform else self.default_augmentation()\n",
    "\n",
    "    def default_augmentation(self):\n",
    "        \"\"\"\n",
    "        Defines the augmentation pipeline for intense augmentation.\n",
    "        \"\"\"\n",
    "        return Compose([\n",
    "            RandomResizedCrop(size=(224, 224), scale=(0.8, 1.2)),  # Random crop and resize\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            RandomRotation(degrees=30),  # Random rotation\n",
    "            ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "            GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2.0)),\n",
    "            RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "        ])\n",
    "\n",
    "    def augment_class(self, label_dir, label, current_count):\n",
    "        \"\"\"\n",
    "        Augments a specific class to reach the target number of images.\n",
    "        Args:\n",
    "            label_dir (str): Directory containing images for the specific class label.\n",
    "            label (str): Name of the label.\n",
    "            current_count (int): Current number of images in the class folder.\n",
    "        \"\"\"\n",
    "        image_files = os.listdir(label_dir)\n",
    "        num_to_augment = self.target_count - current_count\n",
    "\n",
    "        if num_to_augment > 0:\n",
    "            print(f\"Augmenting class '{label}' with {num_to_augment} new images.\")\n",
    "            for i in range(num_to_augment):\n",
    "                img_path = os.path.join(label_dir, image_files[i % len(image_files)])\n",
    "                img = Image.open(img_path).convert(\"RGB\")  # Ensure all images are in RGB mode\n",
    "\n",
    "                # Apply augmentation\n",
    "                augmented_img = self.transform(img)\n",
    "\n",
    "                # Save augmented image\n",
    "                augmented_img_path = os.path.join(label_dir, f\"{label}_aug_{i}.jpg\")\n",
    "                augmented_img.save(augmented_img_path)\n",
    "\n",
    "    def balance_dataset(self):\n",
    "        \"\"\"\n",
    "        Balances all classes by augmenting underrepresented ones.\n",
    "        \"\"\"\n",
    "        action_labels = os.listdir(self.root_dir)\n",
    "        for label in tqdm(action_labels, desc=\"Balancing Classes\"):\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue  # Skip if it's not a directory\n",
    "            current_count = len(os.listdir(label_dir))\n",
    "            if current_count < self.target_count:\n",
    "                self.augment_class(label_dir, label, current_count)\n",
    "\n",
    "# Parameters\n",
    "root_dir = 'action_sp'  # Original dataset path\n",
    "target_count = 100  # Desired number of images per action label\n",
    "\n",
    "# Perform balancing augmentation\n",
    "augmentor = BalancedAugmentor(root_dir, target_count=target_count)\n",
    "augmentor.balance_dataset()\n",
    "\n",
    "# Verify final counts\n",
    "final_counts = {label: len(os.listdir(os.path.join(root_dir, label))) for label in os.listdir(root_dir)}\n",
    "print(f\"Final image counts per class: {final_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize model, loss, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RViT(num_classes=21, hidden_dim=512, num_layers=4, frame_dim=(3, 224, 224)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = RViT(num_classes=5, hidden_dim=128, num_layers=4, frame_dim=(3, 224, 224)).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in train_loader:\n",
    "#         images = images.unsqueeze(2).to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits, _ = model(images)\n",
    "#         loss = criterion(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     scheduler.step()  # Adjust learning rate\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.unsqueeze(2).to(device)  # Add temporal dimension\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2098\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_folder(folder_path, extensions=('jpg', 'jpeg', 'png')):\n",
    "    total_images = 0\n",
    "    \n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(extensions):\n",
    "                total_images += 1\n",
    "                \n",
    "    return total_images\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"action_sp\"  # Replace with your folder path\n",
    "total_images = count_images_in_folder(folder_path)\n",
    "print(f\"Total images: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RViT(num_classes=21, hidden_dim=512, num_layers=4, frame_dim=(3, 224, 224)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/1w8zmpts3jzbt7l9vd8hc7rc0000gn/T/ipykernel_21170/3343109176.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed-precision training\n",
      "Epoch 1/10:   0%|          | 0/36 [00:00<?, ?it/s]/var/folders/7s/1w8zmpts3jzbt7l9vd8hc7rc0000gn/T/ipykernel_21170/3343109176.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 36/36 [07:53<00:00, 13.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 36/36 [08:07<00:00, 13.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 36/36 [08:10<00:00, 13.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.3120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 36/36 [08:04<00:00, 13.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.3115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 36/36 [07:52<00:00, 13.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.3113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 36/36 [07:38<00:00, 12.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 36/36 [08:09<00:00, 13.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.3058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 36/36 [08:35<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 36/36 [08:23<00:00, 13.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.3065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 36/36 [07:58<00:00, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Assuming the model is defined\n",
    "scaler = GradScaler()  # Mixed-precision training\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()  # Replace with your loss function\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients for 4 steps\n",
    "num_epochs = 10\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        if inputs.ndim == 4:  # If depth dimension is missing\n",
    "            inputs = inputs.unsqueeze(2)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):  # Extract logits if model returns a tuple\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        \n",
    "        # Backpropagation with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update weights after gradient accumulation steps\n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RViT(num_classes=21, hidden_dim=512, num_layers=6, frame_dim=(3, 224, 224)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/1w8zmpts3jzbt7l9vd8hc7rc0000gn/T/ipykernel_40316/454544703.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed-precision training\n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Epoch 1/20:   0%|          | 0/210 [00:00<?, ?it/s]/var/folders/7s/1w8zmpts3jzbt7l9vd8hc7rc0000gn/T/ipykernel_40316/454544703.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1/20: 100%|██████████| 210/210 [1:30:29<00:00, 25.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.3170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 210/210 [1:28:28<00:00, 25.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.3099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 210/210 [1:22:57<00:00, 23.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  65%|██████▍   | 136/210 [50:44<27:36, 22.39s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):  \u001b[38;5;66;03m# Extract logits if model returns a tuple\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m, in \u001b[0;36mRViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(lin_attn_output)\u001b[38;5;241m.\u001b[39mto(lin_attn_output\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrvit_units:\n\u001b[0;32m---> 99\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43munit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin_attn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[1;32m    102\u001b[0m h_last \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m))  \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 181\u001b[0m, in \u001b[0;36mRViTUnit.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Attention mechanism with linear attention\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_dropout(attn_output)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Reshape for LayerNorm\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 151\u001b[0m, in \u001b[0;36mLinearAttention.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    149\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), k)  \u001b[38;5;66;03m# Shape: [batch_size, depth*height*width, depth*height*width]\u001b[39;00m\n\u001b[1;32m    150\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m (k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# Scale by sqrt of hidden_dim\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply softmax over last dimension\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Apply attention weights to values\u001b[39;00m\n\u001b[1;32m    154\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(v, attn_weights\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, depth*height*width]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "scaler = GradScaler()  # Mixed-precision training\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()  # Replace with your loss function\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients for 4 stepsscaler = GradScaler()  # Mixed-precision training\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()  # Replace with your loss function\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients for 4 steps\n",
    "\n",
    "# Track epoch losses\n",
    "epoch_losses = []\n",
    "num_epochs = 20\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        if inputs.ndim == 4:  # If depth dimension is missing\n",
    "            inputs = inputs.unsqueeze(2)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):  # Extract logits if model returns a tuple\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        # Backpropagation with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update weights after gradient accumulation steps\n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_losses.append(epoch_loss)  # Save epoch loss\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def visualize_attention_map(frame, attention_weights, output_path=\"attention_map.png\"):\n",
    "#     \"\"\"\n",
    "#     Visualize attention map overlayed on a video frame.\n",
    "#     Args:\n",
    "#         frame: Original frame (H, W, C)\n",
    "#         attention_weights: Attention weights (H, W)\n",
    "#     \"\"\"\n",
    "#     # Normalize attention weights\n",
    "#     attention_weights = (attention_weights - attention_weights.min()) / (attention_weights.max() - attention_weights.min())\n",
    "    \n",
    "#     # Resize attention weights to match the frame size\n",
    "#     attention_resized = cv2.resize(attention_weights, (frame.shape[1], frame.shape[0]))\n",
    "    \n",
    "#     # Create heatmap\n",
    "#     heatmap = cv2.applyColorMap((attention_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    \n",
    "#     # Blend heatmap with original frame\n",
    "#     overlay = cv2.addWeighted(frame, 0.6, heatmap, 0.4, 0)\n",
    "    \n",
    "#     # Save or display the result\n",
    "#     plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(output_path, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# # Example usage\n",
    "# sample_frame = cv2.imread(\"sample_frame.jpg\")  # Replace with an actual frame\n",
    "# sample_attention = np.random.rand(56, 56)  # Replace with actual attention weights\n",
    "# visualize_attention_map(sample_frame, sample_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.unsqueeze(2)  # Adds depth dimension\n",
    "print(inputs.shape)  # Should be [batch_size, channels, depth, height, width]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: ('resized_videos/giving or receiving award/JBWeDivEHFI.mp4',)\n",
      "True Label: 0, Predicted Label: 10\n",
      "Video: ('resized_videos/wrapping present/HscLLuC-PQs.mp4',)\n",
      "True Label: 1, Predicted Label: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_frames, true_label, video_path \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m      7\u001b[0m     video_frames \u001b[38;5;241m=\u001b[39m video_frames\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     10\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 99\u001b[0m, in \u001b[0;36mRViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(lin_attn_output)\u001b[38;5;241m.\u001b[39mto(lin_attn_output\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrvit_units:\n\u001b[0;32m---> 99\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43munit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin_attn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[1;32m    102\u001b[0m h_last \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m))  \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 181\u001b[0m, in \u001b[0;36mRViTUnit.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Attention mechanism with linear attention\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_dropout(attn_output)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Reshape for LayerNorm\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 141\u001b[0m, in \u001b[0;36mLinearAttention.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    139\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWq(x)  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, depth, height, width]\u001b[39;00m\n\u001b[1;32m    140\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWk(h)  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, depth, height, width]\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, depth, height, width]\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Reshape q and k for attention computation\u001b[39;00m\n\u001b[1;32m    144\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, depth*height*width]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    719\u001b[0m     )\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Validate the fine-tuned model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for video_frames, true_label, video_path in test_loader:\n",
    "        video_frames = video_frames.permute(0, 2, 1, 3, 4).to(device)\n",
    "        logits, _ = model(video_frames)\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        total += 1\n",
    "        correct += (predicted_class == true_label.item())\n",
    "        print(f\"Video: {video_path}\")\n",
    "        print(f\"True Label: {true_label.item()}, Predicted Label: {predicted_class}\")\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: ('resized_videos/giving or receiving award/JBWeDivEHFI.mp4',)\n",
      "True Label: giving or receiving award, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/wrapping present/HscLLuC-PQs.mp4',)\n",
      "True Label: wrapping present, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/wrestling/yxJHCSA35Ns.mp4',)\n",
      "True Label: wrestling, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/answering questions/j6ogVLOLQug.mp4',)\n",
      "True Label: answering questions, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/checking tires/pqlY2l0KhEY.mp4',)\n",
      "True Label: checking tires, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/air drumming/5M80ZTWfzOU.mp4',)\n",
      "True Label: air drumming, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/kissing/dNh1nMXCJs8.mp4',)\n",
      "True Label: kissing, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/playing trombone/ScZAUtVAShI.mp4',)\n",
      "True Label: playing trombone, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/milking cow/hbndP8yYwc0.mp4',)\n",
      "True Label: milking cow, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/applauding/GBG55ZwGS-Y.mp4',)\n",
      "True Label: applauding, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/moving furniture/W4EHHzVotV0.mp4',)\n",
      "True Label: moving furniture, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/abseiling/6fBBpbxDuTw.mp4',)\n",
      "True Label: abseiling, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/pushing wheelchair/zsGazDD17iQ.mp4',)\n",
      "True Label: pushing wheelchair, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/riding elephant/9PcEzApDt1g.mp4',)\n",
      "True Label: riding elephant, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/opening bottle/s84sJC48qG4.mp4',)\n",
      "True Label: opening bottle, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/playing harp/FwTXfopti-k.mp4',)\n",
      "True Label: playing harp, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/applying cream/5lrxGVViPFE.mp4',)\n",
      "True Label: applying cream, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/throwing axe/Q54mqN2Sh3I.mp4',)\n",
      "True Label: throwing axe, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/archery/01kR4-7DbN8.mp4',)\n",
      "True Label: archery, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/doing aerobics/yiiBzOiweTU.mp4',)\n",
      "True Label: doing aerobics, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/juggling soccer ball/wGkb460BxKM.mp4',)\n",
      "True Label: juggling soccer ball, Predicted Label: giving or receiving award\n",
      "Video: ('resized_videos/waxing chest/pgGsZrxijcE.mp4',)\n",
      "True Label: waxing chest, Predicted Label: giving or receiving award\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for video_frames, true_label, video_path in test_loader:\n",
    "        video_frames = video_frames.permute(0, 2, 1, 3, 4).to(device)\n",
    "        logits, _ = model(video_frames)\n",
    "        \n",
    "        predicted_class = torch.argmax(logits, dim=1)[0].item()\n",
    "        predicted_folder = test_dataset.label_to_class[predicted_class]\n",
    "        true_folder = test_dataset.label_to_class[true_label.item()]  # Convert tensor to integer\n",
    "\n",
    "        print(f\"Video: {video_path}\")\n",
    "        print(f\"True Label: {true_folder}, Predicted Label: {predicted_folder}\")\n",
    "# Testing loop\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for video_frames, true_label, video_path in test_loader:\n",
    "#         video_frames = video_frames.permute(0, 2, 1, 3, 4).to(device)\n",
    "#         logits, _ = model(video_frames)\n",
    "        \n",
    "#         predicted_class = torch.argmax(logits, dim=1)[0].item()\n",
    "#         predicted_folder = test_dataset.label_to_class[predicted_class]\n",
    "#         true_folder = test_dataset.label_to_class[true_label.item()]  # Convert tensor to integer\n",
    "\n",
    "#         print(f\"Video: {video_path}\")\n",
    "#         print(f\"True Label: {true_folder}, Predicted Label: {predicted_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({1: 3753, 3: 2655, 0: 2258, 4: 2220, 2: 1715})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter(train_dataset.labels)\n",
    "print(f\"Class distribution: {class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RViT(nn.Module):\n",
    "#     def __init__(self, num_classes, hidden_dim, num_layers, frame_dim):\n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         # Patch embedding\n",
    "#         self.patch_embedding = nn.Conv3d(3, hidden_dim, kernel_size=(3, 8, 8), stride=(3, 4, 4), padding=(1, 2, 2))\n",
    "#         self.position_encoding = nn.Parameter(torch.randn(1, hidden_dim, 1, 56, 56), requires_grad=True)\n",
    "\n",
    "#         # Attention layers\n",
    "#         self.scaled_attention = ScaledDotProductAttention(hidden_dim)\n",
    "#         self.linear_attention = LinearAttention(hidden_dim)\n",
    "\n",
    "#         # Recurrent Vision Transformer Units\n",
    "#         self.rvit_units = nn.ModuleList([RViTUnit(hidden_dim) for _ in range(num_layers)])\n",
    "        \n",
    "#         # Classifier\n",
    "#         self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Patch embedding\n",
    "#         patches = self.patch_embedding(x)\n",
    "        \n",
    "#         # Add positional encoding\n",
    "#         _, _, depth, height, width = patches.shape\n",
    "#         pos_encoding = F.interpolate(self.position_encoding, size=(depth, height, width), mode='trilinear', align_corners=False)\n",
    "#         patches += pos_encoding\n",
    "\n",
    "#         # Attention mechanisms\n",
    "#         scaled_attn_output = self.scaled_attention(patches, patches)\n",
    "#         lin_attn_output = self.linear_attention(scaled_attn_output, scaled_attn_output)\n",
    "\n",
    "#         # Recurrent processing\n",
    "#         h = torch.zeros_like(lin_attn_output).to(lin_attn_output.device)\n",
    "#         for unit in self.rvit_units:\n",
    "#             h = unit(lin_attn_output, h)\n",
    "\n",
    "#         # Classification\n",
    "#         h_last = h.mean(dim=(2, 3, 4))  # Global average pooling\n",
    "#         logits = self.classifier(h_last)\n",
    "#         return logits, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RViT(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim, num_layers, frame_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.patch_embedding = nn.Conv3d(3, hidden_dim, kernel_size=(3, 8, 8), stride=(3, 4, 4), padding=(1, 2, 2))\n",
    "        self.position_encoding = nn.Parameter(torch.randn(1, hidden_dim, 1, 56, 56), requires_grad=True)\n",
    "        self.rvit_units = nn.ModuleList([RViTUnit(hidden_dim) for _ in range(num_layers)])\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embedding(x)\n",
    "        _, _, depth, height, width = patches.shape\n",
    "        pos_encoding = F.interpolate(self.position_encoding, size=(depth, height, width), mode='trilinear', align_corners=False)\n",
    "        patches += pos_encoding\n",
    "        h = torch.zeros_like(patches).to(patches.device)\n",
    "        for unit in self.rvit_units:\n",
    "            h = unit(patches, h)\n",
    "        h_last = h.mean(dim=(2, 3, 4))\n",
    "        logits = self.classifier(h_last)\n",
    "        return logits, h\n",
    "\n",
    "class RViTUnit(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.attention_gate = LinearAttention(hidden_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        attn_output = self.attention_gate(x, h)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        batch_size, hidden_dim, depth, height, width = attn_output.shape\n",
    "        h_flat = h.permute(0, 2, 3, 4, 1).reshape(-1, hidden_dim)\n",
    "        attn_output_flat = attn_output.permute(0, 2, 3, 4, 1).reshape(-1, hidden_dim)\n",
    "        h_new_flat = self.layer_norm1(h_flat + attn_output_flat)\n",
    "        h_new = h_new_flat.reshape(batch_size, depth, height, width, hidden_dim).permute(0, 4, 1, 2, 3)\n",
    "        h_new_flat = h_new.permute(0, 2, 3, 4, 1).reshape(-1, hidden_dim)\n",
    "        h_new_flat = self.layer_norm2(h_new_flat + self.ffn(h_new_flat))\n",
    "        h_new = h_new_flat.reshape(batch_size, depth, height, width, hidden_dim).permute(0, 4, 1, 2, 3)\n",
    "        return h_new\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wk = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wv = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        q = self.Wq(x).flatten(start_dim=2)\n",
    "        k = self.Wk(h).flatten(start_dim=2)\n",
    "        v = self.Wv(h).flatten(start_dim=2)\n",
    "        attn_weights = torch.bmm(q.transpose(1, 2), k) / (k.size(1) ** 0.5)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.bmm(v, attn_weights.transpose(1, 2))\n",
    "        return attn_output.view_as(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ActionImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for loading images grouped by classes into subfolders.\n",
    "        Each subfolder represents a class.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []  # List to hold image paths\n",
    "        self.labels = []  # List to hold corresponding class labels\n",
    "        self.classes = sorted(os.listdir(root_dir))  # List of class names (subfolder names)\n",
    "\n",
    "        for label, action in enumerate(self.classes):  # Iterate through subfolders\n",
    "            action_dir = os.path.join(root_dir, action)\n",
    "            if not os.path.isdir(action_dir):\n",
    "                continue  # Skip non-folder files (if any)\n",
    "            for img_file in os.listdir(action_dir):\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Check for image files\n",
    "                    img_path = os.path.join(action_dir, img_file)\n",
    "                    self.data.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "class ActionVideoDatasetSingle(Dataset):\n",
    "    def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(video_dir)\n",
    "        self.video_files = []\n",
    "        for label, cls in enumerate(self.classes):\n",
    "            action_folder = os.path.join(video_dir, cls)\n",
    "            videos = [f for f in os.listdir(action_folder) if f.endswith('.mp4')]\n",
    "            shuffle(videos)\n",
    "            if videos:\n",
    "                self.video_files.append((os.path.join(action_folder, videos[0]), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_files[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while len(frames) < self.num_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ActionImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for loading images directly from a folder containing images (no subfolders).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []  # List to hold image paths\n",
    "        self.labels = []  # List to hold corresponding action labels (optional, can be all 0)\n",
    "\n",
    "        # Single class definition for compatibility\n",
    "        self.classes = ['action_sp']  # Define a single class name\n",
    "\n",
    "        # Collect all .jpg files in the directory\n",
    "        for img_file in os.listdir(root_dir):\n",
    "            if img_file.endswith('.jpg'):\n",
    "                img_path = os.path.join(root_dir, img_file)  # Full path to the image\n",
    "                self.data.append(img_path)\n",
    "                self.labels.append(0)  # Assign a single class label (e.g., 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]  # Will always be 0 in this case\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = Image.fromarray(img)  # Convert NumPy array to PIL image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "class ActionVideoDatasetSingle(Dataset):\n",
    "    def __init__(self, video_dir, num_frames=16, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(video_dir)\n",
    "        self.video_files = []\n",
    "        for label, cls in enumerate(self.classes):\n",
    "            action_folder = os.path.join(video_dir, cls)\n",
    "            videos = [f for f in os.listdir(action_folder) if f.endswith('.mp4')]\n",
    "            shuffle(videos)\n",
    "            if videos:\n",
    "                self.video_files.append((os.path.join(action_folder, videos[0]), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_files[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while len(frames) < self.num_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m Counter(train_dataset\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[1;32m     15\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m class_counts[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mlabels]\n\u001b[0;32m---> 16\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mWeightedRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[1;32m     19\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/sampler.py:263\u001b[0m, in \u001b[0;36mWeightedRandomSampler.__init__\u001b[0;34m(self, weights, num_samples, replacement, generator)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    253\u001b[0m     weights: Sequence[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_samples, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_samples, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m num_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    262\u001b[0m     ):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    264\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         )\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(replacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([Resize((224, 224)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = ActionImageDataset(root_dir='action_sp', transform=transform)\n",
    "test_dataset = ActionVideoDatasetSingle(video_dir='resized_videos', transform=transform)\n",
    "\n",
    "# Weighted Sampler\n",
    "class_counts = Counter(train_dataset.labels)\n",
    "class_weights = [1.0 / class_counts[label] for label in train_dataset.labels]\n",
    "sampler = WeightedRandomSampler(weights=class_weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RViT(num_classes=len(train_dataset.classes), hidden_dim=128, num_layers=4, frame_dim=(3, 224, 224)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.unsqueeze(2).to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Save Pretrained Model\n",
    "torch.save(model.state_dict(), \"rvit_pretrained.pth\")\n",
    "\n",
    "# Fine-Tuning\n",
    "model.load_state_dict(torch.load(\"rvit_pretrained.pth\"))\n",
    "model.train()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for video_frames, labels in test_loader:\n",
    "        video_frames = video_frames.permute(0, 2, 1, 3, 4).to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(video_frames)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Fine-Tune Epoch {epoch+1}, Loss: {running_loss/len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 0\n",
      "Classes: ['action_sp']\n",
      "First few samples: []\n",
      "First few labels: []\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ActionImageDataset(root_dir='action_sp', transform=transform)\n",
    "print(f\"Number of samples in dataset: {len(train_dataset)}\")  # Should be > 0\n",
    "print(f\"Classes: {train_dataset.classes}\")  # Should list all subfolder names\n",
    "print(f\"First few samples: {train_dataset.data[:5]}\")  # Should show file paths\n",
    "print(f\"First few labels: {train_dataset.labels[:5]}\")  # Should show class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
